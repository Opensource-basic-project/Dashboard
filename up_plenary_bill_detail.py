from db import SessionLocal, PlenaryBill
import requests
from bs4 import BeautifulSoup
import time

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def crawl_plenary_review_info(soup):
    def extract(table_summary_keyword, μ²λ¦¬μΌ_idx, μ²λ¦¬κ²°κ³Ό_idx):
        table = soup.find("table", summary=lambda s: s and table_summary_keyword in s)
        if table:
            tr = table.find("tbody").find("tr")
            tds = tr.find_all("td")
            μ²λ¦¬μΌ = tds[μ²λ¦¬μΌ_idx].get_text(strip=True) if len(tds) > μ²λ¦¬μΌ_idx else ""
            μ²λ¦¬κ²°κ³Ό = tds[μ²λ¦¬κ²°κ³Ό_idx].get_text(strip=True) if len(tds) > μ²λ¦¬κ²°κ³Ό_idx else ""
            return μ²λ¦¬μΌ, μ²λ¦¬κ²°κ³Ό
        return "", ""

    # μ†κ΄€μ„ μ‹¬μ‚¬μ •λ³΄: μ²λ¦¬μΌ = td[3], μ²λ¦¬κ²°κ³Ό = td[4]
    so_date, so_result = extract("μ†κ΄€μ„ μ‹¬μ‚¬μ •λ³΄", 3, 4)

    # λ²•μ‚¬μ„ μ‹¬μ‚¬μ •λ³΄: μ²λ¦¬μΌ = td[2], μ²λ¦¬κ²°κ³Ό = td[3]
    law_date, law_result = extract("λ²•μ‚¬μ„ μ²΄κ³„μκµ¬μ‹¬μ‚¬μ •λ³΄", 2, 3)

    # λ³Ένμ μ‹¬μμ •λ³΄: μκ²°μΌ = td[1], νμκ²°κ³Ό = td[3]
    plenary_date, plenary_result = extract("λ³Ένμ μ‹¬μμ •λ³΄", 1, 3)

    return {
        "so_committee_date": so_date,
        "so_committee_result": so_result,
        "law_committee_date": law_date,
        "law_committee_result": law_result,
        "plenary_vote_date": plenary_date,
        "plenary_vote_result": plenary_result
    }

def crawl_plenary_proposal_text(link_url: str):
    try:
        response = requests.get(link_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # β… μ μ•μ΄μ  λ³Έλ¬Έ
        summary_div = soup.find("div", id="summaryContentDiv")
        if summary_div:
            raw_text = summary_div.get_text(separator="\n").strip()
            cleaned_lines = [line.lstrip() for line in raw_text.splitlines()]
            cleaned_text = "\n".join(cleaned_lines)
        else:
            cleaned_text = None

        # β… μ‹¬μ‚¬ μ •λ³΄ ν¬λ΅¤λ§
        review_info = crawl_plenary_review_info(soup)

        return cleaned_text, review_info

    except Exception as e:
        print(f"[μ—λ¬] {link_url} μ²λ¦¬ μ¤‘ μ¤λ¥: {e}")
        return None, {}

def update_plenary_proposal_text():
    db = SessionLocal()
    try:
        targets = db.query(PlenaryBill).filter(
            PlenaryBill.link_url != None
        ).all()

        for bill in targets:
            print(f"[λ³Ένμ] π“„ {bill.bill_name} - ν¬λ΅¤λ§ μ‹λ„ μ¤‘...")
            detail, review_info = crawl_plenary_proposal_text(bill.link_url)

            if not detail:
                print(f"[λ³Ένμ] β 1μ°¨ μ‹¤ν¨, μ¬μ‹λ„ μ¤‘...")
                time.sleep(1)
                detail, review_info = crawl_plenary_proposal_text(bill.link_url)

            if detail:
                changed = False
                if bill.proposal_text != detail:
                    bill.proposal_text = detail
                    changed = True

                # β… μ‹¬μ‚¬μ •λ³΄ μ—…λ°μ΄νΈ
                for key, value in review_info.items():
                    if hasattr(bill, key) and getattr(bill, key) != value:
                        setattr(bill, key, value)
                        changed = True

                if changed:
                    print(f"[μ—…λ°μ΄νΈ] βοΈ {bill.bill_name} - λ³€κ²½ μ‚¬ν•­ λ°μλ¨")
                else:
                    print(f"[μ μ§€] β© {bill.bill_name} - λ‚΄μ© λ™μΌ, λ®μ–΄μ“°κΈ° μƒλµ")
            else:
                print(f"[λ³Ένμ] π« ν¬λ΅¤λ§ μ‹¤ν¨: {bill.link_url}")

            time.sleep(1)

        db.commit()
        print("[λ³Ένμ] β… μ μ•μ΄μ  λ° μ£Όμ”λ‚΄μ© + μ‹¬μ‚¬μ •λ³΄ DB κ°±μ‹  μ™„λ£")

    finally:
        db.close()

if __name__ == "__main__":
    update_plenary_proposal_text()
